{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrbJuQGke4Ltoco8ssgKKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaldounN/Weather-Prediction-/blob/master/FINAL_DSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGLm5GbyVz4h"
      },
      "source": [
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQGX_E-FZe-v",
        "outputId": "2d34bc5c-afae-46a1-8b4c-b9ce8a6769fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ip3 install flair"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: ip3: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V1G-HhOZfBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a968f5ce-809d-4ca9-c7ef-43d1a0ba615a"
      },
      "source": [
        "!python -m spacy download en_core_web_md\r\n"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=60b567a42bad1190a6caa097d5c5344ce14bd6218f8634ef7457ff28c74ba8c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3mg_z2g1/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vhk2aIUgUT7"
      },
      "source": [
        "# Train Model to classify Emotions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZz6aophgTWE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "ae46092f-b009-4902-f06a-461ddfbb403e"
      },
      "source": [
        "# Import data . data from source was saparerated so I can to concatenate it \r\n",
        "\r\n",
        "import pandas as pd \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "df_train = pd.read_csv('/content/train.txt', header =None, sep =';', names = ['Input','Sentiment'], encoding='utf-8')\r\n",
        "df_test = pd.read_csv('/content/test.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')\r\n",
        "df_val = pd.read_csv('/content/val.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')\r\n",
        "df = pd.concat([df_train,df_test, df_val])\r\n",
        "\r\n",
        "\r\n",
        "#check data balance \r\n",
        "df['Sentiment'].value_counts()\r\n",
        "df.head()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Input Sentiment\n",
              "0                            i didnt feel humiliated   sadness\n",
              "1  i can go from feeling so hopeless to so damned...   sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong     anger\n",
              "3  i am ever feeling nostalgic about the fireplac...      love\n",
              "4                               i am feeling grouchy     anger"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4uxaOjYdZYN"
      },
      "source": [
        "# We create our bag of words (bow) using our tokenizer and defining an ngram range\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "\r\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\r\n",
        "cv = CountVectorizer(stop_words='english', ngram_range=(1,1), tokenizer = token.tokenize)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gVrQSXSdZlN"
      },
      "source": [
        "#Custom transformer using Python standard library (you could use spacy as well)\r\n",
        "from sklearn.base import TransformerMixin \r\n",
        "\r\n",
        "# This function will clean the text\r\n",
        "def clean_text(text):     \r\n",
        "    return text.strip().lower()\r\n",
        "    \r\n",
        "#Custom transformer using Python standard library (you could use spacy as well)\r\n",
        "class predictors(TransformerMixin):\r\n",
        "\r\n",
        "    def transform(self, X, **transform_params):\r\n",
        "        return [clean_text(text) for text in X]\r\n",
        "\r\n",
        "    def fit(self, X, y=None, **fit_params):\r\n",
        "        return self\r\n",
        "\r\n",
        "    def get_params(self, deep=True):\r\n",
        "        return {}"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwXBLPfpgTZc"
      },
      "source": [
        "# split data for train and test \r\n",
        "X = df['Input']\r\n",
        "ylabels = df['Sentiment']\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,ylabels, test_size=0.30, random_state=5)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6mUjnQShM_6"
      },
      "source": [
        "# intial training model \r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "\r\n",
        "clf= RandomForestClassifier()"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQDA106Eg5Fi"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\r\n",
        "                 ('vectorizer', cv),\r\n",
        "                 ('classifier', clf)], verbose=True)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkI0iyB0g5NX",
        "outputId": "5323a41d-756a-4e07-f521-b7e1e1ee8504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........... (step 1 of 3) Processing cleaner, total=   0.0s\n",
            "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  18.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f668083d8d0>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words='english', strip_accents=N...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adFBJyNTb_8h"
      },
      "source": [
        "predicted = pipe.predict(X_test)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gGwOJvRb__S"
      },
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj_60CATcABK"
      },
      "source": [
        "acc_score = metrics.accuracy_score(predicted,y_test)\r\n",
        "prec_score = precision_score(y_test,predicted, average='macro')\r\n",
        "recall = recall_score(y_test, predicted,average='macro')\r\n",
        "f1 = f1_score(y_test,predicted,average='macro')\r\n",
        "matrix = confusion_matrix(y_test,predicted)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOY1yiRofJ9o",
        "outputId": "aa2155a5-3168-4461-fddc-e5063d3ff673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(str('Accuracy: '+'{:04.2f}'.format(acc_score*100))+'%')\r\n",
        "print(str('Precision: '+'{:04.2f}'.format(prec_score*100))+'%')\r\n",
        "print(str('Recall: '+'{:04.2f}'.format(recall*100))+'%')\r\n",
        "print('F1 Score: ',f1)\r\n",
        "print(matrix)\r\n"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 88.47%\n",
            "Precision: 85.05%\n",
            "Recall: 85.21%\n",
            "F1 Score:  0.850893769391874\n",
            "[[ 705    9   29    5   48    3]\n",
            " [  42  576   12    2   29   40]\n",
            " [  16   21 1840   73   70   12]\n",
            " [   6    1   90  384    6    2]\n",
            " [  36   35   42    6 1625   10]\n",
            " [   0   22   18    0    7  178]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHRkt0XtfKAp",
        "outputId": "1f77072c-b548-4c16-d109-1855767ade97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_data = ['I dont feel okay ,i am ecstatic my model works', 'I love you', 'i feel lost', 'im petrified', 'I feel frustrated']\r\n",
        "\r\n",
        "test_result = pipe.predict(test_data)\r\n",
        "\r\n",
        "print(test_result)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['joy' 'joy' 'sadness' 'fear' 'anger']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNvBtgLwfKDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FII_h49XfKGZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXHKp1R8fKI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwwFM4L5ZeFc",
        "outputId": "b58bfaca-a4da-484b-9753-5d9dd79cd01e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "from flair.models import TextClassifier\r\n",
        "from flair.data import Sentence\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "plt.style.use(\"seaborn\")\r\n",
        "import math"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-c29f1da65486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhl8ttM0ZeKU",
        "outputId": "da5de719-71b5-4f4f-812e-64832ca495fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.53%\n",
            "Precision: 82.71%\n",
            "Recall: 58.59%\n",
            "F1 Score:  0.6191209661316305\n",
            "[[ 512   14   92    2  179    0]\n",
            " [  26  430   98    2  144    1]\n",
            " [  12   12 1890   23   95    0]\n",
            " [   7    5  241  162   74    0]\n",
            " [  13   11   83    2 1644    1]\n",
            " [   4   29   92    2   84   14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9EgOlBpV0hL"
      },
      "source": [
        "# Defining Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZlBhv_9V7i5"
      },
      "source": [
        "\r\n",
        "# load book , clean data and split into sentences \r\n",
        "def Load_book(path):\r\n",
        "  book = open(path, 'r').read()\r\n",
        "  book = book.replace(\"\\n\",'')\r\n",
        "  sentences  = book.split('.')\r\n",
        "\r\n",
        "  return sentences \r\n",
        "\r\n",
        "# run sentiment analysis on each sentence in the book and save them into df_setiment \r\n",
        "def Analyzse_sentiment(sentences):\r\n",
        "  tagger = TextClassifier.load('sentiment')\r\n",
        "  df_sentiment = pd.DataFrame((np.zeros((4,int(len(sentences))))))\r\n",
        "  for i, sentence in enumerate(sentences):\r\n",
        "    sentence = Sentence(sentence)\r\n",
        "    tagger.predict(sentence)\r\n",
        "    df_sentiment[i].update(sentence.labels)\r\n",
        "\r\n",
        "\r\n",
        "  for i , sentence in enumerate(df_sentiment.iloc[0,:]):\r\n",
        "      try:\r\n",
        "        df_sentiment.iloc[1,i] = sentence.value\r\n",
        "        if sentence.value == 'NEGATIVE':\r\n",
        "          df_sentiment.iloc[2,i] = sentence.score * (-1)\r\n",
        "        else:\r\n",
        "          df_sentiment.iloc[2,i] = sentence.score\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "\r\n",
        "  return df_sentiment\r\n",
        "\r\n",
        "\r\n",
        "# calcualte Major and Minor sentiment and vizualize it in a dashabord style\r\n",
        "\r\n",
        "def Display_sentiment(df_sentiment  , booktitle = ' book title'):\r\n",
        "\r\n",
        "  l = df_sentiment.iloc[2,:]\r\n",
        "  l = pd.DataFrame(l) \r\n",
        "  l['Major'] = l.iloc[:,0].rolling(40).mean()\r\n",
        "  l['Minor'] = l.iloc[:,0].rolling(10).mean()\r\n",
        "\r\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30, 15))\r\n",
        "  axes[0].set_title(booktitle , fontsize = 30 )\r\n",
        "  axes[0].plot(l['Major'])\r\n",
        "  axes[0].set_xlabel('Sentences' , fontsize=20)\r\n",
        "  axes[0].set_ylabel('Sentiment', fontsize=20)\r\n",
        "  axes[0].legend(['Major Sentiment(rolling average for 30 sentences)'], fontsize=20)\r\n",
        "  axes[0].fill_between(l.index, l['Major'] ,0 , where=l['Major'] >= 0, facecolor='lightgreen',  interpolate=True)\r\n",
        "  axes[0].fill_between(l.index, l['Major'] ,0 , where=l['Major'] <= 0, facecolor='lightcoral',  interpolate=True)\r\n",
        "  axes[0].set_facecolor('aliceblue')\r\n",
        "  axes[1].plot(l['Minor'])\r\n",
        "  axes[1].set_xlabel('Sentences' , fontsize=15)\r\n",
        "  axes[1].set_ylabel('Sentiment' , fontsize=15)\r\n",
        "  axes[1].fill_between(l.index, l['Minor'] ,0 , where=l['Minor'] >= 0, facecolor='lightgreen',  interpolate=True)\r\n",
        "  axes[1].fill_between(l.index, l['Minor'] ,0 , where=l['Minor'] <= 0, facecolor='lightcoral',  interpolate=True)\r\n",
        "  axes[1].set_facecolor('aliceblue')\r\n",
        "  axes[1].legend(['Minor Sentiment(rolling average for 5 sentences)'] , fontsize=20)\r\n",
        "\r\n",
        "def Analyzse_emotions(sentences):\r\n",
        "  filename = 'Randomforest_model.sav'\r\n",
        "  clf = pickle.load(open(filename, 'rb'))\r\n",
        "  for i in \r\n",
        "\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762YwF14pdNv"
      },
      "source": [
        "Analyzse_emotions(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt3kyhxTm_Mf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNRDHunAm_wf"
      },
      "source": [
        "\r\n",
        "s = 'I hate you  '\r\n",
        "\r\n",
        "filename = 'Randomforest_model.sav'\r\n",
        "pipe = pickle.load(open(filename, 'rb'))\r\n",
        "f = pipe.predict(s)\r\n",
        "\r\n",
        "print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE2rpIRrWUGb"
      },
      "source": [
        " sentences = Load_book('/content/Beyond good and Evil.txt')\r\n",
        " sentences = sentences["
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJbRRjggXqdH"
      },
      "source": [
        "#df = Analyzse_sentiment(sentences)\r\n",
        "Display_sentiment(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XBczGmYG5i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0FXldpYcmKY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7YZksdi6AAc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCdlCrAVekhq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8IPeKYKsphp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}