{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_5_Classical_Data_Analysis_SVM_EXERCISES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaldounN/Files-/blob/master/Session_5_Classical_Data_Analysis_SVM_EXERCISES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_ck5OSE3TU"
      },
      "source": [
        "![BTS](https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/img/Logo-BTS.jpg)\n",
        "\n",
        "# Session 5: Support Vector Machines EXERCISES\n",
        "\n",
        "### Filipa Peleja <filipa.peleja@bts.tech>\n",
        "### Victor F. Pajuelo Madrigal <victor.pajuelo@bts.tech>\n",
        "\n",
        "## Classical Data Analysis (16-02-2021)\n",
        "\n",
        "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vfp1/bts-cda-2020/blob/master/Session_5/Session_5_Classical_Data_Analysis_SVM_EXERCISES.ipynb)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rDsHQd_aQQ"
      },
      "source": [
        "## Exercise one [NO CODE]\n",
        "\n",
        "\n",
        "\n",
        "1.   What is a support vector?\n",
        "      \n",
        "    -Answer: it's a model that projects non-linear data into different features space to be able to separate them linearly using line, plane, or hyperplane. It can be used for classification and regression---\n",
        "\n",
        "\n",
        "2.   Why it is important to scale inputs when using the SVM?\n",
        "\n",
        "    -Answer: scaling features makes separating them easier and provide better margin: separation is affected by the magnitude and value of features so scaling them levels the plane and makes differences more visible-\n",
        "\n",
        "3. Should you use dual=True or dual=False when training a model with millions of samples but hundreds of features?\n",
        "\n",
        "    -Answer: False \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "razi289oCbos"
      },
      "source": [
        "## Exercise two [NECESSARY]\n",
        "\n",
        "Train a SVM classifier on the datasets shown in class (not the regression one). Take special care with the hyperparameters for multiclassification, C and other hyperparameters that we discussed. You may want to tune the hyperparameters using smaller validation sets to speed up the process. What accuracy can you reach?\n",
        "\n",
        "In this exercise you need to:\n",
        "\n",
        "- Visualize and present the dataset\n",
        "- Apply the SVC to perform binary classification\n",
        "- Comment on the usage of the different kernels and the effect of the hyper-parameters (is always needed a nonlinear kernel?)\n",
        "- Visualize the results with a confusion matrix\n",
        "- Compute the accuracy score and comment on the results\n",
        "- Perform any other experiments that you can think of, always reason about the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FNF98cpEPqf"
      },
      "source": [
        "# Import SVC and datasets from sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = iris[\"target\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtDLTW8lHIeL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain , Xtest , Ytrain, Ytest = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DE3Sm7ZEPuA",
        "outputId": "5beac265-6f44-4513-f282-2d69fd1df4c1"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(Xtrain, Ytrain)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAt0is6CEPxQ"
      },
      "source": [
        "# Now predict the value of test data:\n",
        "expected = Ytest\n",
        "predicted = clf.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hWVonUpHeNk",
        "outputId": "fd12ed03-4c15-4eb5-eeba-3f8cb0e7ddff"
      },
      "source": [
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (clf, metrics.classification_report(expected, predicted)))\n",
        "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report for classifier Pipeline(memory=None,\n",
            "         steps=[('standardscaler',\n",
            "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
            "                ('svc',\n",
            "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
            "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
            "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
            "                     random_state=None, shrinking=True, tol=0.001,\n",
            "                     verbose=False))],\n",
            "         verbose=False):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.92      1.00      0.96        12\n",
            "           2       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[10  0  0]\n",
            " [ 0 12  0]\n",
            " [ 0  1  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw7Ax6QmHeQx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Jsq9DxHeUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_CNk7CCDBZW"
      },
      "source": [
        "## Exercise three [OPTIONAL]\n",
        "\n",
        "*Try to solve this as an optional assignement, we will review the code in the following class*\n",
        "\n",
        "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms. In this exercise, we will develop the intuition behind support vector machines and their use in classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI8A79SnDBZa"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "man6HsMEDBZe"
      },
      "source": [
        "To begin with, let us generate the data for a linear classification problem. In order to do so use the `make_blobs` function from `sklearn`. We want to generate 50 samples, set the `random_state=0` and `cluster_std=0.6`. Finally plot the points with `plt.scatter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUqAe-pRDBZf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IjYXDIVDBZi"
      },
      "source": [
        "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. Think about a line that separates the two classes, how many are there? Which do you think would be the more appropiate given the data points? Draw some lines over this plot with slopes [1, 0.5, -0.2] and biases [0.65, 1.6, 2.9]. Use the `np.linespace` function to generate the x and the line equation \n",
        "\n",
        "$$y = mx + b$$\n",
        "\n",
        "for the y. Finally plot a \"new point\" in the coordinates (0.6, 2.1) with a red X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlK4-WhqDBZj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPmh31QGDBZm"
      },
      "source": [
        "## SVM margins and support vectors\n",
        "These are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!.\n",
        "\n",
        "Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a line between the classes, we can draw around each line a margin of some width, up to the nearest point (no matter the class). To visualize this, let us repeat the same plot but adding some code to fill the margins. Use the method `plt.fill_between` with `color='#AAAAAA'`. The margins for each of the lines above are [0.33, 0.55, 0.2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU89a8OxDBZn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfCWQ4iNDBZq"
      },
      "source": [
        "Now fit an SVM to this data. Use Scikit-Learn's support vector classifier to train an SVM model. For the time being, we will use a linear kernel and set the C parameter to a very large number like 1E10. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSNunBV7DBZr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PezkPYL6DBZt"
      },
      "source": [
        "Now retrieve the support vectors from the learned model. Would you be able to identify them on the plot? Why are these the support vectors? Why are they important?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVi6El7fDBZu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpWuo3Y2DBZx"
      },
      "source": [
        "This function will plot the decision boundaries of a model and the support vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRwkHIUHDBZy"
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, facecolors='none',\n",
        "                   edgecolors='blue')\n",
        "        \n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRWrOwVyDBZ0"
      },
      "source": [
        "Generate a scatter plot of the data set and use the previous function to draw the support vectors and the decision boundaries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UjwQEmtDBZ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uysvg21VDBZ4"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "\n",
        "In order to see an example of this, simulate the points with the same random seed, but now simulate 120. Then train again the SVM and plot the decision boundaries. Which are the support vectors this time? Is this an expected result? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5vprHPDBZ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3iHukpeDBZ7"
      },
      "source": [
        "## SVM softening the margins\n",
        "Now add a new point which is inside the decision boundary, say (-0.5, 2) to class 0, what do you expect will happen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUDN6uIeDBZ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY4n1jerDBZ_"
      },
      "source": [
        "How about adding an even more outlier, like (0, 0) to class 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxD0OxVXDBaA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL4375vTDBaC"
      },
      "source": [
        "What happened to the margin? Could we use this model if we had a red point further right? Why?\n",
        "\n",
        "Now, is there a way that we could try to make the SVM more robust to these possible outliers? Which one? Try to implement an SVM model with the same data modifying the C parameter. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLkzsKUjDBaD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHsrz0xDBaG"
      },
      "source": [
        "## SVM Kernels\n",
        "Now we have seen that SVM are very useful to find the optimal separating hyper-plane when your data is linearly separable. Even when you have some noise in the data set, you can tune the C-value to be able to adjust this. But what happens if your classes are not linearly separable? Is there a way we could overcome this draw-back? Let us generate a dataset that is not linearly separable. Use the `make_circles` method from `samples_generator` in sklearn. Generate 100 examples with `factor=.1, noise=.1, random_state=0`. Then generate a scatter plot to visualize the samples, use a different color for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InJwc2mXDBaH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yn6JyRfDBaJ"
      },
      "source": [
        "Now try to classify this data with a linear SVM, what do you expect will happen? Does this model capture the pattern of the classes? Plot the classification results with the `plot_svc_decision_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bOMWVdSDBaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgCa72_DBaN"
      },
      "source": [
        "What could we do in order to make this dataset linearly separable? We can project it into a higher dimensional space. Note the similarity between this, and the polynomial regression. In order to have a better understanding of how kernels work, first implement a third axis with python with the following expression:\n",
        "\n",
        "$$x_3 = e^{-(x_1^2 + x_2^2)}$$\n",
        "\n",
        "Actually this is somewhat equivalent to the rbf kernel. You should name the new axis `x_3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o45PISlODBaN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiAyuCCEDBaQ"
      },
      "source": [
        "Once you have implemented this 3rd axis, you may "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpDOj7ffDBaR"
      },
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "from ipywidgets import interact, fixed\n",
        "\n",
        "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
        "    ax = plt.subplot(projection='3d')\n",
        "    ax.scatter3D(X[:, 0], X[:, 1], x_3, c=y, s=50, cmap='autumn')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_xlabel('x_1')\n",
        "    ax.set_ylabel('x_2')\n",
        "    ax.set_zlabel('x_3')\n",
        "    \n",
        "interact(plot_3D, elev=[30, 0, 90], azip=(-180, 180),\n",
        "         X=fixed(X), y=fixed(y));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rJsRTQyDBaV"
      },
      "source": [
        "Now use the SVM classifier but set the `kernel='rbf'` with high value for C and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw5G5p5DDBaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEexYnN1DBae"
      },
      "source": [
        "## Evaluating the classification results\n",
        "In order to assess the goodness of our model we need to compute some quantitative scores, we will review some of the most relevant. First, use the function `make_blobs` to generate a multilabel dataset. Generate 200 samples with 4 different classes set `random_state=0` and `cluster_std=0.6` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwvy5D0_DBag"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA2vO9MUDBai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-VV9ldrDBak"
      },
      "source": [
        "Split the data set in training and test and apply the SVC with `C=1, kernel='rbf', gamma='auto', class_weight='balanced', decision_function_shape='ovr'` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XI3eDREDBal"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heL4lVD5DBao"
      },
      "source": [
        "Predict on the test set and plot the confusion matrix, use the `confusion_matrix` function from sklearn and `heatmap` from seaborn. What would be the confusion matrix result of a perfect classification? What information can we extract from it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lbRXQg3DBao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GWVAc9PDBar"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_fpv4dDBat"
      },
      "source": [
        "Finally compute the accuracy score, what does this value mean?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT3eCLRwDBat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWiIwsYeDBav"
      },
      "source": [
        "You can use the follwing code to visualize the decision function computed by the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQYhjHVODBaw"
      },
      "source": [
        "def make_meshgrid(x, y, h=.02):\n",
        "    \"\"\"Create a mesh of points to plot in\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: data to base x-axis meshgrid on\n",
        "    y: data to base y-axis meshgrid on\n",
        "    h: stepsize for meshgrid, optional\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xx, yy : ndarray\n",
        "    \"\"\"\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "\n",
        "\n",
        "def plot_contours(clf, xx, yy, **params):\n",
        "    \"\"\"Plot the decision boundaries for a classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax: matplotlib axes object\n",
        "    clf: a classifier\n",
        "    xx: meshgrid ndarray\n",
        "    yy: meshgrid ndarray\n",
        "    params: dictionary of params to pass to contourf, optional\n",
        "    \"\"\"\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = plt.contourf(xx, yy, Z, **params)\n",
        "    return out\n",
        "\n",
        "X0, X1 = Xtest[:, 0], Xtest[:, 1]\n",
        "xx, yy = make_meshgrid(X0, X1)\n",
        "\n",
        "plot_contours(svc, xx, yy,\n",
        "               cmap='jet', alpha=0.6)\n",
        "\n",
        "plt.scatter(X0, X1, c=ytest, cmap='jet', s=20, edgecolors='k')\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRxrK8DqDBaz"
      },
      "source": [
        "Now repeat the computations with a linear kernel, what changes do you observe? Now change the `cluster_std` when you generate the data, what would you expecte when you increase it? and when you decrease it? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBUs9YaOAEBs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}